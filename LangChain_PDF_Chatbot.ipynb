{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Import the required libraries\n",
    "import os\n",
    "import gradio as gr\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenAI API key is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads the PDF with the info which will be used. this uses the PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step  Load PDF\n",
    "loader = PyPDFLoader(\"sample_annual_report.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is splitting a list of documents into smaller text chunks to make them easier to process (e.g., for embedding or retrieval tasks in NLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code embeds text chunks and stores them in a vector database ChromaDB for retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store with ChromaDB\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma.from_documents(texts, embedding=embeddings)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up a Question-Answering (QA) system using a language model and a retriever:\n",
    "\n",
    "ChatOpenAI(...): Initializes the OpenAI language model (gpt-3.5-turbo) for answering questions.\n",
    "\n",
    "RetrievalQA.from_chain_type(...): Creates a retrieval-augmented QA system that:\n",
    "\n",
    "Uses the retriever to find relevant text chunks, Feeds them to the LLM to generate accurate answers based on context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/lgt9j43560s8c0nc1hvh5yxw0000gn/T/ipykernel_88279/2415109868.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n"
     ]
    }
   ],
   "source": [
    "# Set up QA chain\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/lgt9j43560s8c0nc1hvh5yxw0000gn/T/ipykernel_88279/69710730.py:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain.run(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Response:\n",
      " Key Financial Metrics for 2024:\n",
      "- Total Revenue: $150,000,000\n",
      "- Net Profit: $32,500,000\n",
      "- Operating Margin: 21.7%\n",
      "- Earnings Per Share (EPS): $2.45\n",
      "- Total Employees: 1,250\n",
      "\n",
      "Summary:\n",
      "In 2024, the company achieved significant growth with a 15% increase in revenue driven by strategic investments in technology and international markets. The company reported a total revenue of $150 million, a net profit of $32.5 million, an operating margin of 21.7%, and earnings per share of $2.45. With a positive outlook for 2025, the company plans to continue its growth trajectory by expanding into the Asia-Pacific region and investing further in AI to enhance operational efficiency.\n"
     ]
    }
   ],
   "source": [
    "#  Test Ask a question\n",
    "question = \"Give me the key metrics and a summary in 100 words.\"\n",
    "response = qa_chain.run(question)\n",
    "\n",
    "print(\"Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Step 6: Add Gradio UI for interactive Q&A with prompt options\n",
    "\n",
    "def ask_question_gradio(prompt_choice, user_question):\n",
    "    # Define prompt templates\n",
    "    prompt_templates = {\n",
    "        \"Summary\": \"Please provide a concise summary of the document.\",\n",
    "        \"Extract Metrics\": \"Extract all key numerical metrics from the document.\",\n",
    "        \"Key Insights\": \"What are the main insights and takeaways?\"\n",
    "    }\n",
    "\n",
    "    # Use the selected prompt as a prefix + user question\n",
    "    if user_question.strip():\n",
    "        final_query = f\"{prompt_templates[prompt_choice]} Question: {user_question}\"\n",
    "    else:\n",
    "        final_query = prompt_templates[prompt_choice]\n",
    "\n",
    "    return qa_chain.run(final_query)\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Chat with sample_annual_report.pdf\")\n",
    "\n",
    "    with gr.Row():\n",
    "        prompt_dropdown = gr.Dropdown(\n",
    "            choices=[\"Summary\", \"Extract Metrics\", \"Key Insights\"],\n",
    "            value=\"Summary\",\n",
    "            label=\"Select Prompt Type\"\n",
    "        )\n",
    "        user_input = gr.Textbox(label=\"Ask a question (optional)\")\n",
    "\n",
    "    submit_btn = gr.Button(\"Ask\")\n",
    "    answer_output = gr.Textbox(label=\"Answer\", lines=10)\n",
    "\n",
    "    submit_btn.click(fn=ask_question_gradio, inputs=[prompt_dropdown, user_input], outputs=[answer_output])\n",
    "\n",
    "demo.launch(inline=True)  # inline=True for Jupyter Notebook\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
